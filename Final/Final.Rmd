---
title: "Emre Kemerci  / 311802017 / BDA 503 Fall 2018 Final"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part I

### Q1

I noted my insights for both languages below: 

Python

* I have not enough experience but according to what I heard commonly, Python is more capable in modelling & machine learning
* I think syntax of Python is easier to read and write comparing the R.  R has disadvantage especially due to paranthesis 
* As far as I know, Python is preferred more among data analists/scientists

R

* Better for charming visualisation
* Better reporting capabilities and customization opportunities with Markdown comparing Jupyter notebook
* R Studio looks more friendly for new starters comparing PyCharm
* My insights are that R have variety of libraries, on the other hand coming with consistency problems of the libraries

Opinions stated in the dataschool website commonly overlap with my thoughts but at the end of the day, both languages have advantages and disadavntages. So I think that a data scientist should have capabilities to an extend in both languages and should prefer the one better satisfing the need. 

### Q2

First, the objective - question to be identified - should be clarified. Then I determine what I need to answer the question and continue with understanding what I have on hand. I design the execution of analysis considering the outputs from understanding the data on hand and execute accordingly to reach a conclusion answering the initial objective.

Following the example in the question; I can measure the impact of each project through finding the correlations of overall wellfare with each project. Delta of overall welfare per unit of improvement project. Most probably marginal benefit to overall welfare will decrease and considering the marginal benefit curve can be used to allocate funds efficiently. If I found a clear picture showing which projects should be focused for each welfare levels, info becomes valuable for me. 

I prefer to present what the data says but I must explain the results of the analysis in a meaningful way.

### Q3

I prefer to draw average delay time according to carriers and origins of flight. I choose delay time instead of arrival time because intuitively we know that departure delays  are more than arrival delays. Carriers and the airports are key variables to analyze delay times, that is why I choose these variables.This plot can be used as starting point for following detailed analysis.


```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(nycflights13)
library(ggplot2)
#glimpse(flights)
#summary(flights)
```

```{r}
fl <- flights %>% 
  select(dep_delay, carrier, origin) %>%
  group_by(carrier,origin) %>% 
  summarize(avrdelay=mean(dep_delay, na.rm=TRUE))


ggplot (data=fl, aes(x=carrier, y=avrdelay)) +
  geom_bar(stat='identity', aes(fill=origin)) +
  coord_flip()
```

## Part II

I choose to extend our group term project analysis through comparing the YoY percentage change of total export and import values with change in value of currencies of each countries. The steps of my analysis are as follows:

Loading the required libraries.

```{r, message=FALSE, warning=FALSE}

library(tidyverse)
library(readxl)
library(reshape2)
```

Reading our group project's cleaned the RDS files.

```{r}
githubURL_export <- ("https://github.com/MEF-BDA503/gpj18-group_four/blob/master/export_jTable.rds?raw=true")
githubURL_import <- ("https://github.com/MEF-BDA503/gpj18-group_four/blob/master/import_jTable.rds?raw=true")
githubURL_exportMELTED <- ("https://github.com/MEF-BDA503/gpj18-group_four/blob/master/export_jTableMELTED.rds?raw=true")
githubURL_importMELTED <- ("https://github.com/MEF-BDA503/gpj18-group_four/blob/master/import_jTableMELTED.rds?raw=true")

export_jTable<- readRDS(url(githubURL_export))
import_jTable<- readRDS(url(githubURL_import))
export_jTableMELTED<- readRDS(url(githubURL_exportMELTED))
import_jTableMELTED<- readRDS(url(githubURL_importMELTED))

rm(githubURL_export)
rm(githubURL_import)
rm(githubURL_exportMELTED)
rm(githubURL_importMELTED)
```

I run the report on [TCMB Website](https://evds2.tcmb.gov.tr/index.php?/evds/serieMarket/#collapse_2) to gather closing FX rates of each year from 2013 to 2018 and save as Excel file. The I read this file and manipulated.

```{r}
Fxtable <- read_excel("C:/Users/emrek/Google Drive/BDA/503-EssentialsOfDataAnalytics/GitHub/pj18-EmreKemerci/Final/YoYchFX.xlsx", col_names=TRUE)

Fxtable <- Fxtable %>% melt(id=c("year")) 

colnames(Fxtable) <-  c("Year","Fx","FxYoYChg")

```

I also manipulated trade stat data of our porject

```{r, message=FALSE, warning=FALSE}
tableExport <- export_jTableMELTED %>% 
  group_by(Country, year) %>%
  summarise(totalExpV=sum(Values,na.rm=TRUE)) %>%
  arrange(Country, year) %>%
  mutate(ExpValChng = (totalExpV - lag(totalExpV))/lag(totalExpV)*100)


tableImport <- import_jTableMELTED %>% 
  group_by(Country, year) %>%
  summarise(totalImpV=sum(Values,na.rm=TRUE)) %>%
  arrange(Country, year) %>%
  mutate(ImpValChng = (totalImpV - lag(totalImpV))/lag(totalImpV)*100)

Tradetable <- full_join(tableExport,tableImport) %>%
  select(Country, year, Export="ExpValChng", Import = "ImpValChng") %>%
  melt(id=c("Country", "year")) %>%
  filter(year!=2013)

colnames(Tradetable) <-  c("Country","Year","TradeType","TradeYoYChg")

Tradetable <- Tradetable %>% mutate(Fx = ifelse (Country =="Canada", "CAD",
                                       ifelse(Country=="Russian", "RUB",
                                        ifelse(Country=="China", "CNY",
                                        ifelse(Country=="India", "INR",
                                        ifelse(Country=="Japan", "JPY",
                                        ifelse(Country=="UK", "GBP",
                                        ifelse(Country=="USA", "USD", "EUR"))))))))



Tradetable <- transform(Tradetable, Year = as.numeric(Year))

```

And join the trade stat data with FX data.

```{r, message=FALSE, warning=FALSE}
df <- left_join(Tradetable,Fxtable) 

df <- df %>% select (Country, Fx, Year, FxYoYChg, TradeType, TradeYoYChg) %>%
  dcast(Country+Fx+Year+FxYoYChg~TradeType)
         
colnames(df) <-  c("Country", "Fx", "Year", "FxYoYChg", "ExYoYChg","ImYoYChg")
```

And visualized change in export value, change in import value and change of each countries' FX rate over years according to trade partner countries.

```{r}
ggplot(df, aes(x=Year)) + 
  geom_line(aes(y=FxYoYChg, col="FxYoYChg")) + 
  geom_line(aes(y=ExYoYChg, col="ExYoYChg")) +
  geom_line(aes(y=ImYoYChg, col="ImYoYChg")) +
  facet_wrap(~Country) + 
  labs(subtitle="YoY Changes for Export, Import and FX Rates", 
      y="% Change")
```

Normally, I expect that in times TL depreciated against each countries currencies (hike in green line), export to these countries rises while imports declines.

I see this positive collerated pattern only for India but the other countries do not support this thesis. So, change of FX rate with countries does not enough to explain changes in export and import values.  


## Part 3

```{r, include=FALSE}

library(jsonlite)

```

```{r, include=FALSE, warning=FALSE, message=FALSE}
url <- ("https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2019_limit0_7216a250f6ae72c71cd09563798a9f18.json")


jsonfile <- fromJSON(url) 

data <- lapply(jsonfile[[1]], function(x) {
    x[sapply(x, is.null)] <- NA
    unlist(x)
})

WUR_2019 <- as.data.frame(do.call("cbind", data))
saveRDS(WUR_2019,file="C:/Users/emrek/Google Drive/BDA/503-EssentialsOfDataAnalytics/GitHub/pj18-EmreKemerci/Final/WUR_2019.rds")


url <- ("https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2018_limit0_369a9045a203e176392b9fb8f8c1cb2a.json")

jsonfile <- fromJSON(url) 

data <- lapply(jsonfile[[1]], function(x) {
    x[sapply(x, is.null)] <- NA
   unlist(x)
})


WUR_2018<- as.data.frame(do.call("cbind", data))

saveRDS(WUR_2018,file="C:/Users/emrek/Google Drive/BDA/503-EssentialsOfDataAnalytics/GitHub/pj18-EmreKemerci/Final/WUR_2018.rds")




url <- ("https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2017_limit0_94aa8e595206a1cd1284e2808330a79c.json")

jsonfile <- fromJSON(url) 

data <- lapply(jsonfile[[1]], function(x) {
    x[sapply(x, is.null)] <- NA
   unlist(x)
})


WUR_2017<- as.data.frame(do.call("cbind", data))

saveRDS(WUR_2017,file="C:/Users/emrek/Google Drive/BDA/503-EssentialsOfDataAnalytics/GitHub/pj18-EmreKemerci/Final/WUR_2017.rds")




url <- ("https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2016_limit0_a38b8fe86b742996f9ea3df4b4ca09f3.json")

jsonfile <- fromJSON(url) 

data <- lapply(jsonfile[[1]], function(x) {
    x[sapply(x, is.null)] <- NA
   unlist(x)
})


WUR_2016<- as.data.frame(do.call("cbind", data))

saveRDS(WUR_2016,file="C:/Users/emrek/Google Drive/BDA/503-EssentialsOfDataAnalytics/GitHub/pj18-EmreKemerci/Final/WUR_2016.rds")


url <- ("https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2015_limit0_3b4a26d6f223d75c71def66929572393.json")

jsonfile <- fromJSON(url) 

data <- lapply(jsonfile[[1]], function(x) {
    x[sapply(x, is.null)] <- NA
   unlist(x)
})


WUR_2015<- as.data.frame(do.call("cbind", data))

saveRDS(WUR_2015,file="C:/Users/emrek/Google Drive/BDA/503-EssentialsOfDataAnalytics/GitHub/pj18-EmreKemerci/Final/WUR_2015.rds")


rm(url)
rm(jsonfile)
rm(data)
```



country averages of scores of teaching, research, citation, international outlook and industry income.

I want to investigate how each university scores disperse per country    

```{r, warning=FALSE, message=FALSE}
wur <- readRDS("C:/Users/emrek/Google Drive/BDA/503-EssentialsOfDataAnalytics/GitHub/pj18-EmreKemerci/Final/WUR_2019.rds")

wur <- wur %>% select (name, location, scores_teaching, scores_research, scores_citations, scores_international_outlook, scores_industry_income) %>%
  melt(id=c("name", "location")) 

wur$value <- as.numeric(wur$value)

wur <- wur %>% group_by(variable) %>% mutate(world_avr_score_per_score_type=mean(value))

wur <- wur %>% group_by(location, variable) %>% mutate(location_avr_score_per_score_type=mean(value))

wur <- wur %>% mutate(CountryGroup = ifelse(location_avr_score_per_score_type > world_avr_score_per_score_type, "AboveWorldAverage", "BelowWorldAverage"))

#wurAboveMean <- wur %>% filter(location_avr_score_per_score_type > world_avr_score_per_score_type)
#wurBelowMean <- wur %>% filter(location_avr_score_per_score_type < world_avr_score_per_score_type)

ggplot(wur, aes(value)) +
  geom_density(aes(fill=factor(variable)), alpha=0.8) +
  facet_wrap(~CountryGroup)
 
```

```{r, message=FALSE, warning=FALSE}
wur2 <- wur %>% filter(CountryGroup == "AboveWorldAverage")

ggplot(wur2, aes(value)) +
  geom_density(aes(fill=factor(variable)), alpha=0.8) +
  facet_wrap(~location)

```

